{
  "metadata": {
    "name": "4-1.apartment_transaction_silver",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods._\nimport org.json4s.JsonDSL._\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.Encoder\n\n\nval spark: SparkSession \u003d\n    SparkSession\n        .builder()\n        .appName(\"StatsAnalyzer\")\n        .enableHiveSupport()\n        .config(\"hive.exec.dynamic.partition\", \"true\")\n        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n        .getOrCreate()\n        \n\nval log1\u003dspark.sparkContext.textFile(\"s3://dataeng-handson/logs/*.gz\")\n \n\ncase class Cflog(base_date: String, adid: String, uuid: String,name: String, timestamp: String, gtmTimes: String, screen_name: String\n                , item_id: String, content_type: String, item_category: String, is_zb_agent: String, building_id: String, area_type_id: String\n                , agent_id: String, status: String , button_name: String) \n\ndef parseRawJson(line: String) \u003d {\n     val pieces \u003d line.split(\"\\\\|\") \n     \n     val adid \u003d pieces.apply(1).toString\n     val uuid \u003d pieces.apply(2).toString\n     val name \u003d pieces.apply(3).toString\n     val timestamp \u003d pieces.apply(8).toString\n     val gtmTimes \u003dpieces.apply(7).toString  \n    //JSON Parse\n    val jsonString \u003d pieces.apply(9).toString\n    implicit val formats \u003d DefaultFormats\n    val result \u003d parse(jsonString)\n\n    var screen_name      \u003d (result \\ \"screen_name\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    var item_id           \u003d (result \\ \"item_id\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    val\tcontent_type     \u003d (result \\ \"content_type\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    val\titem_category    \u003d (result \\ \"item_category\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    val\tis_zb_agent      \u003d (result \\ \"is_zb_agent\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    val\tbuilding_id      \u003d (result \\ \"building_id\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    val\tarea_type_id     \u003d (result \\ \"area_type_id\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    val\tagent_id         \u003d (result \\ \"agent_id\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    val\tbutton_name         \u003d (result \\ \"button_name\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    val status  \u003d (result \\ \"status\").extractOpt[String].getOrElse(\"NULL\").replaceAll(\"nil\", \"NULL\")\n    val base_date \u003d timestamp.substring(0,10)\n    \n    \n    Cflog(base_date, adid, uuid, name, timestamp, gtmTimes, screen_name, item_id, content_type, item_category, is_zb_agent, building_id, area_type_id, agent_id, status , button_name)\n}\n\nval logsDFAll \u003d log1.map(line \u003d\u003e parseRawJson(line)).toDF()\nlogsDFAll.createOrReplaceTempView(\"logs\")\nsqlContext.cacheTable(\"logs\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\ncreate database story_data"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "logsDFAll.show"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect count(*) \nfrom logs"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n create external table story_data.applog\n (\n    adid string,\n    uuid string,\n    name string,\n    timestamp string,\n    gtmTimes string,\n    screen_name string,\n    item_id string,\n    content_type string,\n    item_category string,\n    is_zb_agent string,\n    building_id string,\n    area_type_id string,\n    agent_id string,\n    status  string,\n    button_name string\n )\n partitioned by (base_date date)\n STORED AS PARQUET\n LOCATION \u0027s3://dataeng-handson/silver/applog\u0027\n tblproperties (\"parquet.compress\"\u003d\"SNAPPY\" ,\"classification\"\u003d\"parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect * \nfrom story_data.applog"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nshow tables from story_data;"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\ninsert overwrite table   story_data.applog\n    PARTITION(base_date)  \n select \n    adid,\n    uuid,\n    name,\n    timestamp,\n    gtmTimes,\n    screen_name,\n    item_id,\n    content_type,\n    item_category,\n    is_zb_agent,\n    building_id,\n    area_type_id,\n    agent_id,\n    status,\n    button_name,\n    to_date(base_date, \u0027yyyy-MM-dd\u0027) as base_date \nfrom logs"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- 기준일자 ADID 단지ID 이벤트일시\n\n\nselect base_date,\nadid,\nbuilding_id as apart_id,\ntimestamp as base_dt\nfrom story_data.applog\nwhere item_category \u003d \u0027아파트\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- table for number of view per village \n\nCREATE EXTERNAL TABLE  story_data.apart_user_view_silver\n(\n  adid string, \n  apart_id string,\n  base_dt string\n)\nPARTITIONED BY ( \n  base_date date )\nSTORED AS PARQUET\nLOCATION \u0027s3://dataeng-handson/silver/danji_user_view_silver\u0027\ntblproperties (\"parquet.compress\"\u003d\"SNAPPY\" ,\"classification\"\u003d\"parquet\")\n;"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\ninsert overwrite table story_data.apart_user_view_silver\n    PARTITION(base_date)  \nselect \nadid,\nbuilding_id as apart_id,\ntimestamp as base_dt,\nto_date(base_date, \u0027yyyy-MM-dd\u0027) as base_date \nfrom story_data.applog\nwhere item_category \u003d \u0027아파트\u0027\n and building_id !\u003d \u00270\u0027\n and building_id !\u003d \u0027NULL\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect * \nfrom story_data.apart_user_view_silver"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect base_date, apart_id, count(*) as cnt\nfrom story_data.apart_user_view_silver\ngroup by base_date, apart_id\norder by count(*) desc"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect * \nfrom story_data.applog"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- 기준일자 ADID 단지ID 이벤트일시\n\nselect item_id,\ntimestamp as base_dt,\nbuilding_id as apart_id,\nitem_id\nfrom logs\nwhere item_category \u003d \u0027아파트\u0027\nand item_id !\u003d \u0027NULL\u0027\nand building_id !\u003d \u00270\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- table for number of view per apartment\nCREATE EXTERNAL TABLE  story_data.apart_item_view_silver\n(\n  adid string,\n  base_dt string, \n  apart_id string,\n  item_id  string\n)\nPARTITIONED BY ( \n  base_date date )\nSTORED AS PARQUET\nLOCATION \u0027s3://dataeng-handson/silver/apart_item_view_silver\u0027\ntblproperties (\"parquet.compress\"\u003d\"SNAPPY\" ,\"classification\"\u003d\"parquet\")\n;"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\ninsert overwrite table story_data.apart_item_view_silver\n    PARTITION(base_date)  \nselect \nadid,\nbuilding_id as apart_id,\ntimestamp as base_dt,\nitem_id,\nto_date(base_date, \u0027yyyy-MM-dd\u0027) as base_date \nfrom story_data.applog\nwhere item_category \u003d \u0027아파트\u0027\nand item_id !\u003d \u0027NULL\u0027\nand building_id !\u003d \u00270\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nshow tables from story_data"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect *\nfrom logs"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\n    select  \n           hour(timestamp) as h \n           , count(adid) as count\n    from logs \n    where item_category \u003d \u0027아파트\u0027\n    and name \u003d \u0027view_item\u0027\n    and screen_name \u003d \u0027아파트 상세\u0027\n    group by  hour(timestamp)\n    order by  hour(timestamp)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\n\n    select base_date,\n           adid, \n           timestamp as base_dt,\n           building_id as danji_id \n    from logs \n    where item_category \u003d \u0027아파트\u0027\n    and name \u003d \u0027view_item\u0027\n    and screen_name \u003d \u0027아파트 상세\u0027\n \n    \n  "
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n    select *\n    from logs \n    where item_category \u003d \u0027아파트\u0027\n    and item_id !\u003d \u0027NULL\u0027\n  --  and name \u003d \u0027view_item\u0027\n  and screen_name \u003d \u0027아파트 매물상세\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val danjiCountDF \u003d sqlContext.sql(s\"\"\"\n\n    select base_date,\n           adid,\n           timestamp as base_dt,\n           building_id as danji_id\n    from logs \n    where item_category \u003d \u0027아파트\u0027\n    and name \u003d \u0027view_item\u0027\n    and screen_name \u003d \u0027아파트 상세\u0027\n\n\"\"\")\n\n// Parquet 형태로 s3에 저장한다. \ndanjiCountDF.write\n      .mode(\"overwrite\")\n      .format(\"parquet\")\n      .save(\"s3://fc-class/silver/danji_user_view\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val danji_viewDF \u003d spark.read.parquet(\"s3://fc-class/silver/danji_user_view\")\n danji_viewDF.show\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nCREATE EXTERNAL TABLE class.danji_user_view_silver\n(\n  base_dt timestamp, \n  adid string, \n  danji_id int\n)\nPARTITIONED BY ( \n  base_date date \n  )\nSTORED AS PARQUET\nLOCATION \u0027s3://fc-class/hive/silver/danji_user_view\u0027\ntblproperties (\"parquet.compress\"\u003d\"SNAPPY\" ,\"classification\"\u003d\"parquet\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sqlContext.sql(s\"\"\"\n\n   insert overwrite table   class.danji_user_view_silver \n    PARTITION(base_date)  \n    select \n           timestamp as base_dt,\n           adid,\n           building_id as danji_id,\n           base_date\n    from logs \n    where item_category \u003d \u0027아파트\u0027\n    and name \u003d \u0027view_item\u0027\n    and screen_name \u003d \u0027아파트 상세\u0027    \n\n\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect base_date\n      , danji_id\n     , count(*) as danji_view_count\nfrom class.danji_user_view_silver \ngroup by base_date\n        ,danji_id\norder by      count(*) desc    \n"
    }
  ]
}